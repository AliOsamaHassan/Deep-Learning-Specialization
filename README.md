# Deep-Learning-Specialization
Assignments of Deep Learning Specialization(5 courses on Coursera)-Andrew Ng

https://www.deeplearning.ai/

https://www.coursera.org/specializations/deep-learning

Deep Learning 
* [Course 1 Neural Networks and Deep Learning](/Course%201%20Neural%20Networks%20and%20Deep%20Learning)
* [Course 2 Improving Deep Neural Networks](/Course%202%20Improving%20Deep%20Neural%20Networks)
* [Course 3 Structured Machine Learning Projects](/Course%203%20Structured%20Machine%20Learning%20Projects)
* [Course 4 Convolutional Neural Networks](/Course%204%20Convolutional%20Neural%20Networks)
* [Course 5 Sequence Models](/Course%205%20Sequence%20Models)


Course 1: Neural Networks and Deep Learning
### Lecture Handouts:
Handout                | Description
:---:                | :---
[cs229-notes1.pdf](/cs229-notes1.pdf)     | Linear Regression, Classification and logistic regression, Generalized Linear Models
[cs229-notes2.pdf](/cs229-notes2.pdf)     | Generative Learning algorithms
[cs229-notes3.pdf](/cs229-notes3.pdf)     | Support Vector Machines
[cs229-notes4.pdf](/cs229-notes4.pdf)     | Learning Theory
[cs229-notes5.pdf](/cs229-notes5.pdf)     | Regularization and model selection
[cs229-notes6.pdf](/cs229-notes6.pdf)     | 	The perceptron and large margin classifiers
[cs229-notes7a.pdf](/cs229-notes7a.pdf)     | The k-means clustering algorithm
[cs229-notes7b.pdf](/cs229-notes7b.pdf)     | Mixtures of Gaussians and the EM algorithm
[cs229-notes8.pdf](/cs229-notes8.pdf)     | The EM algorithm
[cs229-notes9.pdf](/cs229-notes9.pdf)     | 	Factor analysis
[cs229-notes10.pdf](/cs229-notes10.pdf)     | Principal components analysis
[cs229-notes11.pdf](/cs229-notes11.pdf)     |Independent Components Analysis
[cs229-notes12.pdf](/cs229-notes12.pdf)     | Reinforcement Learning and Control

<table>
   <tr>
      <td>Week 1</td>
      <td>Supervised Learning,</td>
   </tr>
   <tr>
      <td>Why is Deep Learning taking off,</td>
   </tr>
   <tr>
      <td>About this Course,</td>
   </tr>
   <tr>
      <td>Supervised_Learning_for_Neural_Network.</td>
   </tr>
   <tr>
      <td>What_is_Neural_Network,</td>
   </tr>
   <tr>
      <td>What-is-a-NN,</td>
   </tr>
   <tr>
      <td>Why_is_Deep_Learning_Taking_Off</td>
   </tr>
   <tr>
      <td>Week 2</td>
      <td>Binary_Classification,</td>
   </tr>
   <tr>
      <td>C1W2L01 Binary Classification,</td>
   </tr>
   <tr>
      <td>C1W2L02 Logistic Regression,</td>
   </tr>
   <tr>
      <td>C1W2L03 Gradient Descent,</td>
   </tr>
   <tr>
      <td>C1W2L04 Derivatives,</td>
   </tr>
   <tr>
      <td>C1W2L05 Computation Graph,</td>
   </tr>
   <tr>
      <td>C1W2L06 Logistic Regression Gradient descent,</td>
   </tr>
   <tr>
      <td>C1W2L07 Vectorization,</td>
   </tr>
   <tr>
      <td>C1W2L08 Vectorizing Logistic Regression,</td>
   </tr>
   <tr>
      <td>C1W2L09 Broadcasting in Python,</td>
   </tr>
   <tr>
      <td>C1W2L09-2.05.53-A note on python numpy vectors,</td>
   </tr>
   <tr>
      <td>Logistic_Regression,</td>
   </tr>
   <tr>
      <td>Logistic_Regression_Cost_Function,</td>
   </tr>
   <tr>
      <td>untitled-2</td>
   </tr>
   <tr>
      <td>Week 3</td>
      <td>C1W3L01 Neural Networks Overview,</td>
   </tr>
   <tr>
      <td>C1W3L02 Neural Network Representation,</td>
   </tr>
   <tr>
      <td>C1W3L03 Vectorizing across multiple examples,</td>
   </tr>
   <tr>
      <td>C1W3L04 Activation functions,</td>
   </tr>
   <tr>
      <td>C1W3L05 Why do you need non-linear activation functions,</td>
   </tr>
   <tr>
      <td>C1W3L06 Derivatives of activation functions,</td>
   </tr>
   <tr>
      <td>C1W3L07 Gradient descent for neural networks,</td>
   </tr>
   <tr>
      <td>C1W3L08 Backpropagation intuition (Optional),</td>
   </tr>
   <tr>
      <td>C1W3L09 Random Initialization</td>
   </tr>
   <tr>
      <td>Week 4</td>
      <td>C1W4L02 Getting your matrix dimensions right,</td>
   </tr>
   <tr>
      <td>C1W4L03 Why deep representations,</td>
   </tr>
   <tr>
      <td>C1W4L04 Building blocks of deep neural networks,</td>
   </tr>
   <tr>
      <td>C1W4L05 Parameters vs Hyperparameters,</td>
   </tr>
   <tr>
      <td>C1W4L06 What does this have to do with the brain,</td>
   </tr>
   <tr>
      <td>C1W4L06-ForwardBackProp_annotated</td>
   </tr>
</table>
